{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 774\n",
      "Rate limit reached. Sleeping for: 766\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Twitter API credentials\n",
    "api_key = \"ufmmiP6WPrf88KFWBucUYqyqj\"\n",
    "api_secret = \"VC5lt1Wa6Y4TDsMByVSzRe5iB36XXNMdFrD99yibpTKt4rHtXS\"\n",
    "access_token = \"1483027022645706754-4JCnTuUQ8FfUMBS8MOre8cufJBK2O6\"\n",
    "access_token_secret = \"5y0pofXx8zwSNJwCWaPFRK04KYli2E3MAsP29lpsB2tZO\"\n",
    "\n",
    "# Authenticate to Twitter API\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create API object\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open(\"1000_1500.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "    # Loop through the rows of the CSV file\n",
    "    \n",
    "    for row in reader:\n",
    "        latitude = row[0]\n",
    "        longitude = row[1]\n",
    "        # Search for tweets by location using the Cursor object\n",
    "        try:\n",
    "            tweets = tweepy.Cursor(api.search_tweets, q=\"\", geocode=f\"{latitude},{longitude},1km\", count=1000).items()\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"Rate limit exceeded. Waiting for 15 minutes...\")\n",
    "            time.sleep(900)\n",
    "            continue\n",
    "        \n",
    "        with open(\"tweetsspaces2.csv\", \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            # Write the header row\n",
    "            writer.writerow([\"Latitude\", \"Longitude\",\"Date\", \"User\", \"Text\"])\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                if tweet.coordinates is not None:\n",
    "                    # Write the tweet text, user name, latitude, and longitude to the CSV file\n",
    "                    writer.writerow([tweet.coordinates[\"coordinates\"][1], tweet.coordinates[\"coordinates\"][0],tweet.created_at, tweet.user.screen_name, tweet.text])\n",
    "                        \n",
    "    time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# The input CSV file\n",
    "input_file = \"tweets.csv\"\n",
    "\n",
    "# The output CSV file\n",
    "output_file = \"Istanbultweets.csv\"\n",
    "\n",
    "# Regular expression pattern to match URLs\n",
    "url_pattern = re.compile(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\")\n",
    "\n",
    "# Read the input CSV file\n",
    "with open(input_file, \"r\") as f_input:\n",
    "    reader = csv.reader(f_input)\n",
    "    header = next(reader)\n",
    "    text_column_index = header.index(\"Text\")\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Split the text and URLs into separate columns\n",
    "rows = []\n",
    "for row in data:\n",
    "    text = row[text_column_index]\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    at_index = text.find(\"@\")\n",
    "    if at_index != -1:\n",
    "        text_before_at = text[:at_index].strip()\n",
    "        text_after_at = text[at_index:].strip()\n",
    "    else:\n",
    "        text_before_at = text\n",
    "        text_after_at = \"\"\n",
    "    for url in urls:\n",
    "        text_before_at = text_before_at.replace(url[0] + \"://\" + url[1] + url[2], \"\")\n",
    "        text_before_at = re.sub(r'[^\\w\\s]', '', text_before_at) # remove symbols\n",
    "        text_before_at = re.sub(r'#\\w+', '', text_before_at) # remove hashtags\n",
    "        rows.append(row[:text_column_index] + [text_before_at, text_after_at] + [url[0] + \"://\" + url[1] + url[2]] + row[text_column_index+1:])\n",
    "\n",
    "# Write the output CSV file\n",
    "with open(output_file, \"w\", newline=\"\") as f_output:\n",
    "    writer = csv.writer(f_output)\n",
    "    writer.writerow(header + [\"Text after @\", \"URL\"])\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "820a2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Users/mansikothari/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans) (2021.10.8)\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: sniffio in /Users/mansikothari/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15735 sha256=484baf5b4714f867a573484c20a5425a7b811e30cc6a822e83417606e6ae97f6\n",
      "  Stored in directory: /Users/mansikothari/Library/Caches/pip/wheels/27/f3/32/d4859d40071f07a5df0ab6fdc0076e78a8a786625dde2b4b2f\n",
      "Successfully built googletrans\n",
      "Installing collected packages: hyperframe, hpack, h2, h11, rfc3986, idna, httpcore, hstspreload, chardet, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\u001b[0m\n",
      "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8191ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from googletrans import Translator\n",
    "\n",
    "# The input CSV file\n",
    "input_file = \"output.csv\"\n",
    "\n",
    "# The output CSV file\n",
    "output_file = \"English01.csv\"\n",
    "\n",
    "# Read the input CSV file\n",
    "with open(input_file, \"r\") as f_input:\n",
    "    reader = csv.reader(f_input)\n",
    "    header = next(reader)\n",
    "    text_column_index = header.index(\"Text\")\n",
    "    data = [row for row in reader]\n",
    "    \n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translator = Translator(service_urls=['translate.google.com'])\n",
    "        translated_text = translator.translate(text, dest='en').text\n",
    "        return translated_text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Write the output CSV file\n",
    "with open(output_file, \"w\", newline=\"\") as f_output:\n",
    "    writer = csv.writer(f_output)\n",
    "    writer.writerow(header)\n",
    "    for row in data[:100]:\n",
    "        text = row[text_column_index]\n",
    "        translated_text = translate_text(text)\n",
    "        row[text_column_index] = translated_text\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620aba18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
